import os
import glob
import whisper
import tiktoken
import torch
from openai import OpenAI

# ğŸ” ClÃ© API OpenAI en dur (âš ï¸ garde Ã§a confidentiel)
client = OpenAI(api_key="sk-etc")

# ğŸ“ CrÃ©ation des dossiers de sortie
os.makedirs("transcriptions", exist_ok=True)
os.makedirs("comptes_rendus", exist_ok=True)

# ğŸ” Recherche des fichiers audio
audio_files = glob.glob("*.mp3") + glob.glob("*.mp4")
if not audio_files:
    print("âŒ Aucun fichier audio trouvÃ©.")
    exit()

# ğŸš€ Chargement du modÃ¨le Whisper avec dÃ©tection auto GPU/CPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Chargement du modÃ¨le Whisper sur : {device.upper()}")
model = whisper.load_model("small", device=device)

# âš™ï¸ ParamÃ¨tres de dÃ©coupage et tokenisation
encoding = tiktoken.get_encoding("cl100k_base")
max_chunk_tokens = 10000
max_response_tokens = 2000
MAX_TOTAL_TOKENS = 128000

# ğŸ” Traitement de chaque fichier audio
for audio_file in audio_files:
    print(f"\nğŸµ Fichier dÃ©tectÃ© : {audio_file}")
    print("â³ Transcription en cours...")

    try:
        result = model.transcribe(audio_file)
        transcription = result["text"]
    except Exception as e:
        print(f"âŒ Erreur lors de la transcription : {e}")
        continue

    # ğŸ’¾ Sauvegarde de la transcription
    base_name = os.path.splitext(audio_file)[0]
    transcript_path = os.path.join("transcriptions", f"{base_name}_transcription.txt")
    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(transcription)
    print(f"âœ… Transcription enregistrÃ©e : {transcript_path}")

    # âœ‚ï¸ DÃ©coupage de la transcription si besoin
    def split_text(text, max_tokens):
        words = text.split()
        chunks = []
        current = []
        count = 0

        for word in words:
            t = len(encoding.encode(word + " "))
            if count + t > max_tokens:
                chunks.append(" ".join(current))
                current = [word]
                count = t
            else:
                current.append(word)
                count += t

        if current:
            chunks.append(" ".join(current))
        return chunks

    chunks = split_text(transcription, max_chunk_tokens)
    print(f"ğŸ“š Transcription dÃ©coupÃ©e en {len(chunks)} sections.")

    partial_summaries = []

    # ğŸ¤– RÃ©sumÃ© de chaque chunk
    for i, chunk in enumerate(chunks):
        print(f"ğŸ§  GÃ©nÃ©ration du rÃ©sumÃ© pour la section {i+1}/{len(chunks)}...")

        try:
            response = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un assistant qui rÃ©dige des comptes rendus professionnels de cours et coaching."
                    },
                    {
                        "role": "user",
                        "content": f"Voici une portion de la transcription approximative d'un cours. Fais un compte rendu structurÃ© :\n\n{chunk}"
                    }
                ],
                temperature=0.7,
                max_tokens=max_response_tokens
            )

            summary = response.choices[0].message.content.strip()
            partial_summaries.append(summary)

            # ğŸ”¸ Sauvegarde du rÃ©sumÃ© partiel
            part_path = os.path.join("comptes_rendus", f"{base_name}_part_{i+1}.txt")
            with open(part_path, "w", encoding="utf-8") as f:
                f.write(summary)

        except Exception as e:
            print(f"âŒ Erreur section {i+1} : {e}")

    # ğŸ§© Fusion des rÃ©sumÃ©s partiels
    final_report = "\n\n---\n\n".join(partial_summaries)
    final_path = os.path.join("comptes_rendus", f"{base_name}_compte_rendu.txt")
    with open(final_path, "w", encoding="utf-8") as f:
        f.write(final_report)

    print(f"âœ… Compte rendu final gÃ©nÃ©rÃ© : {final_path}")

